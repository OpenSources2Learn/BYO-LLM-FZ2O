| Title| Year |Resources|
| ------- | ----- | ------ |
|[FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)|Axiv2022|[[Paper]](https://arxiv.org/abs/2205.14135) ,[[Code]](https//github.com/openai/triton/blob/main/python/triton/ops/flash_attention.py) ,[[Note]](https://mp.weixin.qq.com/s/1WH_7FWpGTJG5mUZ95o2-A)|
|[SWITCHHEAD: ACCELERATING TRANSFORMERS WITH MIXTURE-OF-EXPERTS ATTENTION](https://arxiv.org/pdf/2312.07987.pdf)|Arxiv2023|[[Paper]](https://arxiv.org/pdf/2312.07987.pdf) ,[[Code]](https://github.com/robertcsordas/moe_attention)|
|[Efficient Streaming Language Models with Attention Sinks](https://arxiv.org/abs/2309.17453)|Arxiv2023|[[Paper]](https://arxiv.org/abs/2309.17453) ,[[Code]](https://github.com/mit-han-lab/streaming-llm)|
|[LLM Augmented LLMs: Expanding Capabilities through Composition](https://arxiv.org/abs/2401.02412)|Arxiv2024|[[Paper]](https://arxiv.org/abs/2401.02412) ,[[Code]](https://github.com/lucidrains/CALM-pytorch)|

