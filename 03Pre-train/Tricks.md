| Title| Year |Paper|
| ------- | ----- | ------ |
|[FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)|Axiv2022|[[Paper]](https://arxiv.org/abs/2205.14135) ,[[Code]](https//github.com/openai/triton/blob/main/python/triton/ops/flash_attention.py) ,[[Note]](https://mp.weixin.qq.com/s/1WH_7FWpGTJG5mUZ95o2-A)|
|[SWITCHHEAD: ACCELERATING TRANSFORMERS WITH MIXTURE-OF-EXPERTS ATTENTION](https://arxiv.org/pdf/2312.07987.pdf)|Arxiv2023|[[Paper]](https://arxiv.org/pdf/2312.07987.pdf) ,[[Code]](https://github.com/robertcsordas/moe_attention)|
