| Title| Year |Resources|
| ------- | ----- | ------ |
|[Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache](https://arxiv.org/pdf/2401.02669.pdf)|Arxiv2024|[[Paper]](https://arxiv.org/pdf/2401.02669.pdf) ,[[Note]](https://mp.weixin.qq.com/s/TAx3UEy10tlHphqccwQEVw)|
|[LLM-Pruner](https://arxiv.org/abs/2305.11627)|NeurIPS2023|[[Paper]](https://arxiv.org/abs/2305.11627) ,[[Code]](https://github.com/horseee/LLM-Pruner)|
|[GPTQ: Accurate Post-training Quantization of Generative Pretrained Transformers](https://arxiv.org/abs/2210.17323)|ICLR2023|[[Paper]](https://arxiv.org/abs/2210.17323) ,[[Code]](https://github.com/IST-DASLab/gptq) ,[[GPTQ-for-LLaMa]](https://github.com/qwopqwop200/GPTQ-for-LLaMa)|
|[Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study](https://arxiv.org/abs/2307.08072)|Arxiv2023|[[Paper]](https://arxiv.org/abs/2307.08072) ,[[Code]](https://github.com/RUCAIBox/QuantizedEmpirical)|
